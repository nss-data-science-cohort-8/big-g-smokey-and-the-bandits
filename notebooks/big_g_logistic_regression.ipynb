{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    FunctionTransformer\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Much of this code borrowed from Andrew, as it's cleaner and faster, thanks Andrew!)  \n",
    "##### Load and prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults_raw = pd.read_csv(\"../data/J1939Faults.csv\", dtype={\"EquipmentID\": str, 'spn':int})\n",
    "diagnostics_raw = pd.read_csv(\"../data/vehiclediagnosticonboarddata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults_raw['EventTimeStamp'] = pd.to_datetime(faults_raw['EventTimeStamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare faults\n",
    "faults_drop_cols = [\"actionDescription\", \"activeTransitionCount\", \"ecuSource\", \"ecuSoftwareVersion\", \"ecuModel\", \"ecuMake\", \"faultValue\", \"MCTNumber\", \"LocationTimeStamp\"]\n",
    "faults = faults_raw.drop(columns=faults_drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join diagnostics\n",
    "diagnostics_raw[\"Value\"] = diagnostics_raw[\"Value\"].replace(\n",
    "    {\"FALSE\": False, \"TRUE\": True}\n",
    ")\n",
    "\n",
    "# pivot diagnostics to long format\n",
    "diagnostics = diagnostics_raw.pivot(\n",
    "    index=\"FaultId\", columns=\"Name\", values=\"Value\"\n",
    ")\n",
    "\n",
    "joined = faults.merge(diagnostics, how = \"inner\", left_on='RecordID', right_on='FaultId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Borrowing from Jeff)\n",
    "##### Adding some feature engineering for severity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_severity(text):\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "        \n",
    "    # \"Severity\" followed by \"Low\", \"Medium\", or \"High\"\n",
    "    pattern = r'Severity\\s+(Low|Medium|High)'\n",
    "    \n",
    "    # Search for the pattern \n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        # Return \"Severity\" plus the matched level\n",
    "        return f\"Severity {match.group(1)}\"\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to create the new column\n",
    "joined['SeverityLevel'] = joined['eventDescription'].apply(extract_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_map = {\n",
    "    'Severity Low': 1,\n",
    "    'Severity Medium': 2,\n",
    "    'Severity High': 3\n",
    "}\n",
    "\n",
    "joined['SeverityLevelFeature'] = joined['SeverityLevel'].map(severity_map)\n",
    "\n",
    "joined.loc[joined['spn'] == 1569, 'SeverityLevelFeature'] = 4\n",
    "joined.loc[joined['spn'] == 5394, 'SeverityLevelFeature'] = 4\n",
    "joined.loc[joined['spn'] == 4094, 'SeverityLevelFeature'] = 4\n",
    "joined.loc[joined['spn'] == 5246, 'SeverityLevelFeature'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter out faults near service stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out near service stations\n",
    "joined_pre_station_filter = joined\n",
    "stations = pd.DataFrame(\n",
    "    {\n",
    "        \"lat\": [36.0666667, 35.5883333, 36.1950],\n",
    "        \"lon\": [-86.4347222, -86.4438888, -83.174722],\n",
    "    }\n",
    ")\n",
    "threshold_miles = 0.5\n",
    "threshold_meters = threshold_miles * 1609.34\n",
    "# create geodataframes with geopandas\n",
    "gdf_joined = gpd.GeoDataFrame(\n",
    "    joined,\n",
    "    geometry=gpd.points_from_xy(joined.Latitude, joined.Longitude),\n",
    "    crs=\"EPSG:4326\",  # WGS84 coord ref sys (lat/lon)\n",
    ")\n",
    "gdf_stations = gpd.GeoDataFrame(\n",
    "    stations,\n",
    "    geometry=gpd.points_from_xy(stations.lat, stations.lon),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "target_crs = \"EPSG:9311\"\n",
    "# reproject onto new crs for better distance measurement\n",
    "gdf_joined_proj = gdf_joined.to_crs(target_crs)\n",
    "gdf_stations_proj = gdf_stations.to_crs(target_crs)\n",
    "# create buffers around stations\n",
    "station_buf = gdf_stations_proj.geometry.buffer(threshold_meters)\n",
    "combined_buffer = (\n",
    "    station_buf.union_all()\n",
    ")  # turns into single geometry which helps with efficiency\n",
    "is_within = gdf_joined_proj.geometry.within(combined_buffer)\n",
    "joined[\"nearStation\"] = is_within.values\n",
    "joined_post_filter = joined[~joined[\"nearStation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ns = joined[joined['nearStation'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning up dataframe, and converting data to usable types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ns = joined_ns.drop(columns = ['ESS_Id'\n",
    "                                      , 'ecuSerialNumber'\n",
    "                                      , 'AcceleratorPedal'\n",
    "                                      , 'CruiseControlActive'\n",
    "                                      , 'CruiseControlSetSpeed'\n",
    "                                      , 'Latitude'\n",
    "                                      , 'Longitude'\n",
    "                                      , 'IgnStatus'\n",
    "                                      , 'ParkingBrake'\n",
    "                                      , 'LampStatus'\n",
    "                                      , 'ServiceDistance'\n",
    "                                      , 'nearStation'\n",
    "                                      , 'eventDescription'\n",
    "                                      , 'SeverityLevel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['BarometricPressure', 'DistanceLtd',\n",
    "       'EngineCoolantTemperature', 'EngineLoad', 'EngineOilPressure',\n",
    "       'EngineOilTemperature', 'EngineRpm', 'EngineTimeLtd', 'FuelLevel',\n",
    "       'FuelLtd', 'FuelRate', 'FuelTemperature', 'IntakeManifoldTemperature', \n",
    "        'Speed', 'SwitchedBatteryVoltage', 'Throttle', 'TurboBoostPressure']\n",
    "\n",
    "joined_ns[cols] = joined_ns[cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again borrowing this bit from Andrew)  \n",
    "##### Adding in a derate window which includes a 5246 derate event and any event within a 2-hr window preceding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\n",
    "    \"RecordID\",\n",
    "    \"EquipmentID\",\n",
    "    \"EventTimeStamp\",\n",
    "    \"spn\",\n",
    "    \"fmi\",\n",
    "    \"SeverityLevelFeature\",\n",
    "    \"active\",\n",
    "    \"BarometricPressure\",\n",
    "    \"DistanceLtd\",\n",
    "    \"EngineCoolantTemperature\",\n",
    "    \"EngineLoad\",\n",
    "    \"EngineOilPressure\",\n",
    "    \"EngineOilTemperature\",\n",
    "    \"EngineRpm\",\n",
    "    \"EngineTimeLtd\",\n",
    "    \"FuelLevel\",\n",
    "    \"FuelLtd\",\n",
    "    \"FuelRate\",\n",
    "    \"FuelTemperature\",\n",
    "    \"IntakeManifoldTemperature\",\n",
    "    \"Speed\",\n",
    "    \"SwitchedBatteryVoltage\",\n",
    "    \"Throttle\",\n",
    "    \"TurboBoostPressure\"\n",
    "]\n",
    "target_spn = 5246\n",
    "\n",
    "# --- SORTING STEP ---\n",
    "# Sort by EquipmentID and then chronologically by EventTimeStamp\n",
    "joined_ns = joined_ns.sort_values(by=[\"EquipmentID\", \"EventTimeStamp\"]).copy()\n",
    "\n",
    "# Create a Series containing only the timestamps of trigger events\n",
    "trigger_timestamps_only = joined_ns[\"EventTimeStamp\"].where(joined_ns[\"spn\"] == target_spn)\n",
    "\n",
    "# For each row, find the timestamp of the *next* trigger event within its group\n",
    "# Group by EquipmentID and use backward fill (bfill)\n",
    "# This fills NaT values with the next valid timestamp in the group\n",
    "joined_ns[\"next_trigger_time\"] = trigger_timestamps_only.groupby(\n",
    "    joined_ns[\"EquipmentID\"]\n",
    ").bfill()\n",
    "\n",
    "# Calculate the start of the 2-hour window before the next trigger\n",
    "joined_ns[\"window_start_time\"] = joined_ns[\"next_trigger_time\"] - pd.Timedelta(hours=5)\n",
    "\n",
    "# Label rows as True if their timestamp falls within the window:\n",
    "#    [window_start_time, next_trigger_time]\n",
    "#    Also ensure that a next_trigger_time actually exists (it's not NaT)\n",
    "joined_ns[\"derate_window\"] = (\n",
    "    (joined_ns[\"EventTimeStamp\"] >= joined_ns[\"window_start_time\"])\n",
    "    & (joined_ns[\"EventTimeStamp\"] <= joined_ns[\"next_trigger_time\"])\n",
    "    & (joined_ns[\"next_trigger_time\"].notna())\n",
    ")\n",
    "\n",
    "# Drop helper columns if no longer needed\n",
    "joined_ns = joined_ns.drop(columns=['next_trigger_time', 'window_start_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering out any active = False events (as they mark the end of an event, rather than a fault coming on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ns_act = joined_ns[joined_ns['active'] == True]\n",
    "joined_ns_act = joined_ns_act.drop(columns = ['active'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turning the derate window from a boolean to numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ns_act['derate'] = np.where(joined_ns_act['derate_window'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separating training and test data by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_ns_act['IsTestData'] = joined_ns_act['EventTimeStamp'] >= '2019-01-01'\n",
    "train_data = joined_ns_act[joined_ns_act['IsTestData'] == False]\n",
    "test_data = joined_ns_act[joined_ns_act['IsTestData'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 321\n",
    "\n",
    "X_train = train_data.drop(columns = ['RecordID'\n",
    "                                     , 'EventTimeStamp'\n",
    "                                     , 'EquipmentID'\n",
    "                                     , 'derate_window'\n",
    "                                     , 'derate'\n",
    "                                     , 'IsTestData'])\n",
    "y_train = train_data['derate']\n",
    "\n",
    "X_test = test_data.drop(columns = ['RecordID'\n",
    "                                   , 'EventTimeStamp'\n",
    "                                   , 'EquipmentID'\n",
    "                                   , 'derate_window'\n",
    "                                   , 'derate'\n",
    "                                   , 'IsTestData'])\n",
    "y_test = test_data['derate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SimpleImputer().fit(X_train)\n",
    "\n",
    "X_train = si.transform(X_train)\n",
    "X_test = si.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#removed, as PCA makes results worse\n",
    "#pca = PCA(n_components = 3)\n",
    "#X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "#X_test_scaled = pca.transform(X_test_scaled)\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', random_state=random_state)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=lr_model.classes_,\n",
    "            yticklabels=lr_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = test_data.copy()\n",
    "analysis['prediction'] = y_pred\n",
    "analysis['probability'] = lr_model.predict_proba(X_test_scaled)[:,1] #grab predicted probability of all derates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison, creating a dataframe of derates after 2019 (after eliminating 'duplicate' derates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derates_2019 = joined_ns[(joined_ns['spn'] == 5246) & (joined_ns['EventTimeStamp'] > '12-31-2018') & (joined_ns['active'] == True)].copy()\n",
    "derates_2019['derate_gap'] = derates_2019.sort_values(by=['EquipmentID', 'EventTimeStamp']).groupby('EquipmentID')['EventTimeStamp'].diff()\n",
    "gap = pd.to_timedelta(\"24 hours\")\n",
    "confirmed_derates_2019 = derates_2019[(derates_2019.derate_gap.isnull()) | (derates_2019['derate_gap'] > gap)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repurposing Andrew's earlier code to find gaps between derate and prediction, eliminate 'duplicate' positives (those predicting same event) and anything within the derate window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_analysis_temp = analysis[(analysis['derate'] == 1.0) & (analysis['prediction'] == 1.0)]\n",
    "tp_analysis_temp = tp_analysis_temp[['EventTimeStamp', 'spn', 'EquipmentID', 'derate_window', 'derate', 'prediction', 'probability']]\n",
    "\n",
    "target_spn = 5246\n",
    "\n",
    "tp_analysis_temp = tp_analysis_temp.sort_values(by=[\"EquipmentID\", \"EventTimeStamp\"]).copy()\n",
    "\n",
    "trigger_timestamps_only = tp_analysis_temp[\"EventTimeStamp\"].where(tp_analysis_temp[\"spn\"] == target_spn)\n",
    "\n",
    "tp_analysis_temp[\"next_trigger_time\"] = trigger_timestamps_only.groupby(\n",
    "    tp_analysis_temp[\"EquipmentID\"]\n",
    ").bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_analysis = tp_analysis_temp.copy()\n",
    "tp_analysis['trigger_gap'] = tp_analysis['next_trigger_time'] - tp_analysis['EventTimeStamp']\n",
    "trigger_gap = pd.to_timedelta(\"2 hours\")\n",
    "tp_analysis = tp_analysis[(tp_analysis['trigger_gap'] > trigger_gap)]\n",
    "tp_analysis.drop_duplicates(subset=['next_trigger_time'], keep = 'first', inplace = True)\n",
    "tp_analysis = tp_analysis[(tp_analysis['spn'] != 5246) & (tp_analysis.next_trigger_time.notnull())]\n",
    "tp_analysis['derate_gap'] = tp_analysis.sort_values(by=['EquipmentID', 'EventTimeStamp']).groupby('EquipmentID')['EventTimeStamp'].diff()\n",
    "gap = pd.to_timedelta(\"24 hours\")\n",
    "true_positives = tp_analysis[(tp_analysis.derate_gap.isnull()) | (tp_analysis['derate_gap'] > gap)]\n",
    "true_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Culling repeat false positives (those predicting same event):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_analysis = analysis[(analysis['derate'] == 0.0) & (analysis['prediction'] == 1.0)]\n",
    "fp_analysis = fp_analysis[['EventTimeStamp', 'spn', 'EquipmentID', 'derate_window', 'derate', 'prediction', 'probability']]\n",
    "fp_analysis['derate_gap'] = fp_analysis.sort_values(by=['EquipmentID', 'EventTimeStamp']).groupby('EquipmentID')['EventTimeStamp'].diff()\n",
    "gap = pd.to_timedelta(\"24 hours\")\n",
    "false_positives = fp_analysis[(fp_analysis.derate_gap.isnull()) | (fp_analysis['derate_gap'] > gap)]\n",
    "false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now to watch the money pour in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives.shape[0] * 4000 - false_positives.shape[0] * 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...oh..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
